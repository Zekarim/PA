{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#-------------------------\n",
    "# LOAD AND PREP THE DATA\n",
    "#-------------------------\n",
    " \n",
    "raw_data = pd.read_csv('data_10.csv')\n",
    "raw_data.columns = ['user', 'video', 'score']\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = raw_data.dropna()\n",
    "\n",
    "# Convert names into numerical IDs\n",
    "data['user_id'] = data['user'].astype(\"category\").cat.codes\n",
    "data['video_id'] = data['video'].astype(\"category\").cat.codes\n",
    "\n",
    "# Create a lookup frame so we can get the video names back in \n",
    "# readable form later.\n",
    "item_lookup = data[['video_id', 'video']].drop_duplicates()\n",
    "item_lookup['video_id'] = item_lookup.video_id.astype(str)\n",
    "\n",
    "data = data.drop(['user', 'video'], axis=1)\n",
    "\n",
    "# Create lists of all users, videos and plays\n",
    "users = list(np.sort(data.user_id.unique()))\n",
    "videos = list(np.sort(data.video_id.unique()))\n",
    "scores = list(data.score)\n",
    "\n",
    "# Get the rows and columns for our new matrix\n",
    "rows = data.user_id.astype(int)\n",
    "cols = data.video_id.astype(int)\n",
    "\n",
    "# Contruct a sparse matrix for our users and items containing number of plays\n",
    "data_sparse = sparse.csr_matrix((scores, (rows, cols)), shape=(len(users), len(videos)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def implicit_als(sparse_data, alpha_val=40, iterations=10, lambda_val=0.1, features=10):\n",
    "    \"\"\" Implementation of Alternating Least Squares with implicit data. We iteratively\n",
    "    compute the user (x_u) and item (y_i) vectors using the following formulas:\n",
    "\n",
    "    x_u = ((Y.T*Y + Y.T*(Cu - I) * Y) + lambda*I)^-1 * (X.T * Cu * p(u))\n",
    "    y_i = ((X.T*X + X.T*(Ci - I) * X) + lambda*I)^-1 * (Y.T * Ci * p(i))\n",
    "\n",
    "    Args:\n",
    "        sparse_data (csr_matrix): Our sparse user-by-item matrix\n",
    "\n",
    "        alpha_val (int): The rate in which we'll increase our confidence\n",
    "        in a preference with more interactions.\n",
    "\n",
    "        iterations (int): How many times we alternate between fixing and \n",
    "        updating our user and item vectors\n",
    "\n",
    "        lambda_val (float): Regularization value\n",
    "\n",
    "        features (int): How many latent features we want to compute.\n",
    "\n",
    "    Returns:     \n",
    "        X (csr_matrix): user vectors of size users-by-features\n",
    "        \n",
    "        Y (csr_matrix): item vectors of size items-by-features\n",
    "        \"\"\"\n",
    "\n",
    "    # Calculate the concidence for each value in our data\n",
    "    confidence = sparse_data * alpha_val\n",
    "\n",
    "    # Get the size of user rows and item columns\n",
    "    user_size, item_size = sparse_data.shape\n",
    "\n",
    "    # We create the user vectors X of size users-by-features, the item vectors\n",
    "    # Y of size items-by-features and randomly assign the values.\n",
    "    X = sparse.csr_matrix(np.random.normal(size = (user_size, features)))\n",
    "    Y = sparse.csr_matrix(np.random.normal(size = (item_size, features)))\n",
    "\n",
    "    #Precompute I and lambda * I\n",
    "    X_I = sparse.eye(user_size)\n",
    "    Y_I = sparse.eye(item_size)\n",
    "\n",
    "    I = sparse.eye(features)\n",
    "    lI = lambda_val * I\n",
    "\n",
    "    # Start main loop. For each iteration we first compute X and then Y\n",
    "    for i in range(iterations):0\n",
    "        print('iteration %d of %d' % (i+1, iterations))\n",
    "        \n",
    "        # Precompute Y-transpose-Y and X-transpose-X\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "\n",
    "        # Loop through all users\n",
    "        for u in range(user_size):\n",
    "\n",
    "            # Get the user row.\n",
    "            u_row = confidence[u,:].toarray() \n",
    "\n",
    "            # Calculate the binary preference p(u)\n",
    "            p_u = u_row.copy()\n",
    "            p_u[p_u != 0] = 1.0\n",
    "\n",
    "            # Calculate Cu and Cu - I\n",
    "            CuI = sparse.diags(u_row, [0])\n",
    "            Cu = CuI + Y_I\n",
    "\n",
    "            # Put it all together and compute the final formula\n",
    "            yT_CuI_y = Y.T.dot(CuI).dot(Y)\n",
    "            yT_Cu_pu = Y.T.dot(Cu).dot(p_u.T)\n",
    "            X[u] = spsolve(yTy + yT_CuI_y + lI, yT_Cu_pu)\n",
    "\n",
    "    \n",
    "        for i in range(item_size):\n",
    "\n",
    "            # Get the item column and transpose it.\n",
    "            i_row = confidence[:,i].T.toarray()\n",
    "\n",
    "            # Calculate the binary preference p(i)\n",
    "            p_i = i_row.copy()\n",
    "            p_i[p_i != 0] = 1.0\n",
    "\n",
    "            # Calculate Ci and Ci - I\n",
    "            CiI = sparse.diags(i_row, [0])\n",
    "            Ci = CiI + X_I\n",
    "\n",
    "            # Put it all together and compute the final formula\n",
    "            xT_CiI_x = X.T.dot(CiI).dot(X)\n",
    "            xT_Ci_pi = X.T.dot(Ci).dot(p_i.T)\n",
    "            Y[i] = spsolve(xTx + xT_CiI_x + lI, xT_Ci_pi)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 20\n",
      "iteration 2 of 20\n",
      "iteration 3 of 20\n",
      "iteration 4 of 20\n",
      "iteration 5 of 20\n",
      "iteration 6 of 20\n",
      "iteration 7 of 20\n",
      "iteration 8 of 20\n",
      "iteration 9 of 20\n",
      "iteration 10 of 20\n",
      "iteration 11 of 20\n",
      "iteration 12 of 20\n",
      "iteration 13 of 20\n",
      "iteration 14 of 20\n",
      "iteration 15 of 20\n",
      "iteration 16 of 20\n",
      "iteration 17 of 20\n",
      "iteration 18 of 20\n",
      "iteration 19 of 20\n",
      "iteration 20 of 20\n"
     ]
    }
   ],
   "source": [
    "user_vecs, item_vecs = implicit_als(data_sparse, iterations=20, features=20, alpha_val=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         video     score\n",
      "0  pvcrJLIQ38k  1.000000\n",
      "1  mj27OU__BRs  0.957050\n",
      "2  RSUUR7qR-IM  0.875314\n",
      "3  Iv0N5HSz6FA  0.801588\n",
      "4  RA84xvOiP5A  0.783485\n",
      "5  LSaSG4n3T6o  0.747321\n",
      "6  IV3dnLzthDA  0.746427\n",
      "7  vYb3rB0jU70  0.746325\n",
      "8  yv96bNSPBlY  0.740269\n",
      "9  oI_X2cMHNe0  0.739745\n"
     ]
    }
   ],
   "source": [
    "# Let's say we want to recommend videos for user with ID 2023\n",
    "user_id = 2\n",
    "\n",
    "#------------------------------\n",
    "# GET ITEMS CONSUMED BY USER\n",
    "#------------------------------\n",
    "\n",
    "# Let's print out what the user has listened to\n",
    "consumed_idx = data_sparse[user_id,:].nonzero()[1].astype(str)\n",
    "consumed_items = item_lookup.loc[item_lookup.video_id.isin(consumed_idx)]\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "# CREATE USER RECOMMENDATIONS\n",
    "#------------------------------\n",
    "\n",
    "def recommend(user_id, data_sparse, user_vecs, item_vecs, item_lookup, num_items=10):\n",
    "    \"\"\"Recommend items for a given user given a trained model\n",
    "    \n",
    "    Args:\n",
    "        user_id (int): The id of the user we want to create recommendations for.\n",
    "        \n",
    "        data_sparse (csr_matrix): Our original training data.\n",
    "        \n",
    "        user_vecs (csr_matrix): The trained user x features vectors\n",
    "        \n",
    "        item_vecs (csr_matrix): The trained item x features vectors\n",
    "        \n",
    "        item_lookup (pandas.DataFrame): Used to map video ids to video names\n",
    "        \n",
    "        num_items (int): How many recommendations we want to return:\n",
    "        \n",
    "    Returns:\n",
    "        recommendations (pandas.DataFrame): DataFrame with num_items video names and scores\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    # Get all interactions by the user\n",
    "    user_interactions = data_sparse[user_id,:].toarray()\n",
    "\n",
    "    # We don't want to recommend items the user has consumed. So let's\n",
    "    # set them all to 0 and the unknowns to 1.\n",
    "    user_interactions = user_interactions.reshape(-1) + 1 #Reshape to turn into 1D array\n",
    "    user_interactions[user_interactions > 1] = 0\n",
    "\n",
    "    # This is where we calculate the recommendation by taking the \n",
    "    # dot-product of the user vectors with the item vectors.\n",
    "    rec_vector = user_vecs[user_id,:].dot(item_vecs.T).toarray()\n",
    "\n",
    "    # Let's scale our scores between 0 and 1 to make it all easier to interpret.\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    recommend_vector = user_interactions*rec_vector_scaled\n",
    "   \n",
    "    # Get all the video indices in order of recommendations (descending) and\n",
    "    # select only the top \"num_items\" items. \n",
    "    item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
    "\n",
    "    videos = []\n",
    "    scores = []\n",
    "\n",
    "    # Loop through our recommended video indicies and look up the actial video name\n",
    "    for idx in item_idx:\n",
    "        videos.append(item_lookup.video.loc[item_lookup.video_id == str(idx)].iloc[0])\n",
    "        scores.append(recommend_vector[idx])\n",
    "\n",
    "    # Create a new dataframe with recommended video names and scores\n",
    "    recommendations = pd.DataFrame({'video': videos, 'score': scores})\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Let's generate and print our recommendations\n",
    "recommendations = recommend(user_id, data_sparse, user_vecs, item_vecs, item_lookup)\n",
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
