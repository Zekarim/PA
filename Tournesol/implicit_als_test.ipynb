{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea74e32e8d1541278f16de55ca194779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import implicit\n",
    "\n",
    "# Load the data like we did before\n",
    "raw_data = pd.read_csv('data_60.csv')\n",
    "raw_data.columns = ['user', 'artist', 'plays']\n",
    "\n",
    "# Drop NaN columns\n",
    "data = raw_data.dropna()\n",
    "data = data.copy()\n",
    "\n",
    "# Create a numeric user_id and artist_id column\n",
    "data['user'] = data['user'].astype(\"category\")\n",
    "data['artist'] = data['artist'].astype(\"category\")\n",
    "data['user_id'] = data['user'].cat.codes\n",
    "data['artist_id'] = data['artist'].cat.codes\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# The implicit library expects data as an item-user matrix, so we\n",
    "# create two matrices: one for fitting the model (item-user) \n",
    "# and one for recommendations (user-item)\n",
    "sparse_item_user_train = sparse.csr_matrix((train_data['plays'].astype(float), (train_data['artist_id'], train_data['user_id'])))\n",
    "sparse_item_user_test = sparse.csr_matrix((test_data['plays'].astype(float), (test_data['artist_id'], test_data['user_id'])))\n",
    "\n",
    "sparse_user_item_train = sparse.csr_matrix((train_data['plays'].astype(float), (train_data['user_id'], train_data['artist_id'])))\n",
    "sparse_user_item_test = sparse.csr_matrix((test_data['plays'].astype(float), (test_data['user_id'], test_data['artist_id'])))\n",
    "\n",
    "\n",
    "# Initialize the als model and fit it using the sparse item-user matrix\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=100)\n",
    "\n",
    "# Calculate the confidence by multiplying it by our alpha value.\n",
    "alpha_val = 200\n",
    "data_conf = (sparse_item_user_train * alpha_val).astype('double')\n",
    "\n",
    "#Fit the model\n",
    "model.fit(data_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  (0, 42)\t81.5\n",
      "  (0, 62)\t92.18\n",
      "  (0, 129)\t-37.02\n",
      "  (0, 261)\t-63.88\n",
      "  (0, 290)\t80.99\n",
      "  (0, 483)\t81.29\n",
      "  (0, 537)\t80.78\n",
      "  (0, 648)\t-72.1\n",
      "  (0, 719)\t83.12\n",
      "  (0, 792)\t-55.69\n",
      "  (0, 809)\t-66.18\n",
      "  (0, 857)\t75.8\n",
      "  (0, 865)\t-6.55\n",
      "  (0, 916)\t91.24\n",
      "  (0, 947)\t10.18\n",
      "  (0, 955)\t83.93\n",
      "  (0, 972)\t-26.8\n",
      "  (0, 1008)\t89.17\n",
      "  (0, 1013)\t88.74\n",
      "  (0, 1018)\t75.8\n",
      "  (0, 1076)\t76.05\n",
      "  (0, 1203)\t88.1\n",
      "  (0, 1219)\t78.11\n",
      "  (0, 1237)\t75.8\n",
      "  (0, 1330)\t86.97\n",
      "  :\t:\n",
      "  (0, 11452)\t-86.45\n",
      "  (0, 11463)\t-68.65\n",
      "  (0, 11465)\t58.76\n",
      "  (0, 11557)\t86.85\n",
      "  (0, 11569)\t88.26\n",
      "  (0, 11741)\t76.17\n",
      "  (0, 11765)\t36.54\n",
      "  (0, 11860)\t9.25\n",
      "  (0, 11889)\t91.93\n",
      "  (0, 12013)\t-62.14\n",
      "  (0, 12030)\t31.1\n",
      "  (0, 12048)\t83.77\n",
      "  (0, 12096)\t63.97\n",
      "  (0, 12178)\t74.35\n",
      "  (0, 12429)\t67.98\n",
      "  (0, 12461)\t46.15\n",
      "  (0, 12499)\t92.82\n",
      "  (0, 12511)\t68.63\n",
      "  (0, 12598)\t84.86\n",
      "  (0, 12656)\t-68.77\n",
      "  (0, 12673)\t77.82\n",
      "  (0, 12680)\t92.09\n",
      "  (0, 12791)\t53.39\n",
      "  (0, 12861)\t84.59\n",
      "  (0, 12872)\t86.79\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 129 is out of bounds for axis 1 with size 105",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb Cellule 2\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Call evaluate_model to evaluate your model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m evaluation_results \u001b[39m=\u001b[39m evaluate_model(model, sparse_user_item_test, sparse_user_item_train, k)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Print the evaluation results\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEvaluation results:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb Cellule 2\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_data, train_data, k)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(train_data[\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Make recommendations for the current user\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m recommended_items \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrecommend(user, train_data[\u001b[39m0\u001b[39;49m], N\u001b[39m=\u001b[39;49mk)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m# Get the recommended item IDs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/zekarim/Documents/Master/PA/tournesol/implicit_example.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m recommended_items \u001b[39m=\u001b[39m [item[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m recommended_items]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/implicit/cpu/matrix_factorization_base.py:79\u001b[0m, in \u001b[0;36mMatrixFactorizationBase.recommend\u001b[0;34m(self, userid, user_items, N, filter_already_liked_items, filter_items, recalculate_user, items)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[39mif\u001b[39;00m items \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         filter_query_items \u001b[39m=\u001b[39m _filter_items_from_sparse_matrix(items, filter_query_items)\n\u001b[0;32m---> 79\u001b[0m ids, scores \u001b[39m=\u001b[39m topk(\n\u001b[1;32m     80\u001b[0m     item_factors,\n\u001b[1;32m     81\u001b[0m     user,\n\u001b[1;32m     82\u001b[0m     N,\n\u001b[1;32m     83\u001b[0m     filter_query_items\u001b[39m=\u001b[39;49mfilter_query_items,\n\u001b[1;32m     84\u001b[0m     filter_items\u001b[39m=\u001b[39;49mfilter_items,\n\u001b[1;32m     85\u001b[0m     num_threads\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_threads,\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misscalar(userid):\n\u001b[1;32m     89\u001b[0m     ids, scores \u001b[39m=\u001b[39m ids[\u001b[39m0\u001b[39m], scores[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32mtopk.pyx:32\u001b[0m, in \u001b[0;36mimplicit.cpu.topk.topk\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtopk.pyx:54\u001b[0m, in \u001b[0;36mimplicit.cpu.topk._topk_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 129 is out of bounds for axis 1 with size 105"
     ]
    }
   ],
   "source": [
    "# Set the value of k for precision@k and recall@k\n",
    "k = 5\n",
    "\n",
    "# Call evaluate_model to evaluate your model\n",
    "evaluation_results = evaluate_model(model, sparse_user_item_test, sparse_user_item_train, k)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation results:\")\n",
    "for metric, value in evaluation_results.items():\n",
    "    print(\"{}: {}\".format(metric, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(recommended_items, relevant_items, k):\n",
    "    # Get the top-k recommended items\n",
    "    top_k_items = recommended_items[:k]\n",
    "    # Calculate the number of recommended items that are relevant\n",
    "    num_relevant_items = len(set(top_k_items) & set(relevant_items))\n",
    "    # Calculate precision@k\n",
    "    precision = num_relevant_items / k\n",
    "    return precision\n",
    "\n",
    "def recall_at_k(recommended_items, relevant_items, k):\n",
    "    # Get the top-k recommended items\n",
    "    top_k_items = recommended_items[:k]\n",
    "    # Calculate the number of relevant items that are recommended\n",
    "    num_relevant_items = len(set(top_k_items) & set(relevant_items))\n",
    "    # Calculate recall@k\n",
    "    recall = num_relevant_items / len(relevant_items)\n",
    "    return recall\n",
    "\n",
    "def average_precision(recommended_items, relevant_items):\n",
    "    precision_sum = 0.0\n",
    "    num_relevant_items = 0\n",
    "    for i, item in enumerate(recommended_items):\n",
    "        if item in relevant_items:\n",
    "            num_relevant_items += 1\n",
    "            precision = precision_at_k(recommended_items, relevant_items, i+1)\n",
    "            precision_sum += precision\n",
    "    if num_relevant_items == 0:\n",
    "        return 0.0\n",
    "    average_precision = precision_sum / num_relevant_items\n",
    "    return average_precision\n",
    "\n",
    "def evaluate_model(model, test_data, train_data, k):\n",
    "    # Create a dictionary to store evaluation results\n",
    "    evaluation_results = {}\n",
    "\n",
    "    # Iterate over each user in the test data\n",
    "    for user in range(train_data.shape[0]):\n",
    "        print(user)\n",
    "        # Get the items in the test set for the current user\n",
    "        relevant_items = test_data[user].indices\n",
    "\n",
    "        # Get the items that were already interacted with in the training set\n",
    "        interacted_items = train_data[user].indices\n",
    "\n",
    "        # Remove the already interacted items from relevant items\n",
    "        relevant_items = list(set(relevant_items) - set(interacted_items))\n",
    "\n",
    "        # Make recommendations for the current user\n",
    "        recommended_items = model.recommend(user, train_data[0], N=k)\n",
    "\n",
    "        # Get the recommended item IDs\n",
    "        recommended_items = [item[0] for item in recommended_items]\n",
    "\n",
    "        # Calculate precision@k\n",
    "        precision = precision_at_k(recommended_items, relevant_items, k)\n",
    "\n",
    "        # Calculate recall@k\n",
    "        recall = recall_at_k(recommended_items, relevant_items, k)\n",
    "\n",
    "        # Calculate average precision\n",
    "        avg_precision = average_precision(recommended_items, relevant_items)\n",
    "\n",
    "        # Store the evaluation results for the current user\n",
    "        evaluation_results[user] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'avg_precision': avg_precision\n",
    "        }\n",
    "\n",
    "    # Calculate the average evaluation metrics across all users\n",
    "    precision_values = [result['precision'] for result in evaluation_results.values()]\n",
    "    recall_values = [result['recall'] for result in evaluation_results.values()]\n",
    "    avg_precision_values = [result['avg_precision'] for result in evaluation_results.values()]\n",
    "\n",
    "    avg_precision = np.mean(avg_precision_values)\n",
    "    mean_precision = np.mean(precision_values)\n",
    "    mean_recall = np.mean(recall_values)\n",
    "\n",
    "    return {\n",
    "        'precision@{}'.format(k): mean_precision,\n",
    "        'recall@{}'.format(k): mean_recall,\n",
    "        'MAP@{}'.format(k): avg_precision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 121.62299999931363\n",
      "RMSE: 136.5501838513273\n"
     ]
    }
   ],
   "source": [
    "user_id = 1\n",
    "recommended = model.recommend(userid=user_id, user_items=sparse_item_user_test[user_id], N=10)\n",
    "\n",
    "actual_ratings = []\n",
    "predicted_ratings = []\n",
    "\n",
    "for artist_id in recommended[0]:\n",
    "    # Find the corresponding score in the data DataFrame\n",
    "    data_score = data.loc[data['artist_id'] == artist_id, 'plays'].iloc[0]\n",
    "\n",
    "    # Find the corresponding score in the recommended list\n",
    "    recommended_index = np.where(recommended[0] == artist_id)[0]\n",
    "    recommended_score = recommended[1][recommended_index][0] * 200 - 100\n",
    "\n",
    "    actual_ratings.append(data_score)\n",
    "    predicted_ratings.append(recommended_score)\n",
    "\n",
    "    \n",
    "#Compute evaluation metrics\n",
    "\n",
    "mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
    "rmse = mean_squared_error(actual_ratings, predicted_ratings, squared=False)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 135.95598010760546\n",
      "RMSE: 141.13467700356972\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "actual_ratings = []\n",
    "predicted_ratings = []\n",
    "\n",
    "for artist_id in recommended[0]:\n",
    "    # Find the corresponding score in the data DataFrame\n",
    "    data_score = data.loc[data['artist_id'] == artist_id, 'plays'].iloc[0]\n",
    "\n",
    "    # Find the corresponding score in the recommended list\n",
    "    recommended_index = np.where(recommended[0] == artist_id)[0]\n",
    "    recommended_score = recommended[1][recommended_index][0] * 200 - 100\n",
    "\n",
    "    actual_ratings.append(data_score)\n",
    "    predicted_ratings.append(recommended_score)\n",
    "\n",
    "    \n",
    "#Compute evaluation metrics\n",
    "\n",
    "mae = mean_absolute_error(actual_ratings, predicted_ratings)\n",
    "rmse = mean_squared_error(actual_ratings, predicted_ratings, squared=False)\n",
    "\n",
    "print(\"MAE:\", mae)\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_item_dict = { v:k for k,v in item_dict.items()}\n",
    "for item_index, predicted_score in zip(*recommended_items):\n",
    "    print(f\"https://tournesol.app/entities/yt:{reversed_item_dict[item_index]}\", f\"Predicted score: {predicted_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
